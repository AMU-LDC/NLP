{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">**Bootcamp Big Data & Machine Learning V</p>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<p style=\"text-align: center;\">NLP - Práctica Final</p>**</br>\n",
    "**<p style=\"text-align: center;\">Alberto Muñoz Freán</p>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Análisis de Sentimiento:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En segundo lugar, generaremos un modelo de clasificación de binaria capaz de separar las reviews de nuestro dataset de Amazon en dos categorías: buenas (3 a 5 estrellas) o malas (1 o 2 estrellas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos todo lo necesario para el análisis de sentimiento:\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import LdaModel, CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos los archivos json que contienen las reviews en sendos dataframe:\n",
    "reviews_tools = pd.read_json('Tools_and_Home_Improvement_5.json', orient='columns', lines=True)\n",
    "reviews_phones = pd.read_json('Cell_Phones_and_Accessories_5.json', orient='columns', lines=True)\n",
    "reviews_clothing = pd.read_json('Clothing_Shoes_and_Jewelry_5.json', orient='columns', lines=True)\n",
    "reviews_sports = pd.read_json('Sports_and_Outdoors_5.json', orient='columns', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296337, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unimos los cuatro subsets en un solo data frame:\n",
    "df1 = reviews_tools.combine_first(reviews_phones)\n",
    "df2 = df1.combine_first(reviews_clothing)\n",
    "df3 = df2.combine_first(reviews_sports)\n",
    "\n",
    "#Hacemos un shuffle de todos los resultados, de modo que estén mezclados y no por orden de entrada en el dataset conjunto:\n",
    "df_full = df3.sample(frac=1)\n",
    "\n",
    "#Confirmamos el resultado:\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amfre\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Works as expected - it is the five mode versio...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am kind of disappointed with this item. It w...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not only was the product just as nice as the p...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Bolse AON6 battery pack is a basic medium ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I needed to replace the Siemens switch on my I...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  Works as expected - it is the five mode versio...      5.0\n",
       "1  I am kind of disappointed with this item. It w...      3.0\n",
       "2  Not only was the product just as nice as the p...      5.0\n",
       "3  The Bolse AON6 battery pack is a basic medium ...      3.0\n",
       "4  I needed to replace the Siemens switch on my I...      3.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Del dataframe completo, solo nos quedamos con el texto de la review (reviewText) y las estrellas (overall):\n",
    "df_final = df_full[[\"reviewText\",\"overall\"]]\n",
    "\n",
    "#Eliminamos los nulos, incompatibles con el modelo:\n",
    "df_final.dropna(inplace=True)\n",
    "\n",
    "#Reiniciamos los índices (cada registro mantenía el índice heredado del primer dataframe, hecho con los archivos json):\n",
    "df_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Comprobamos el resultado:\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amfre\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Works as expected - it is the five mode versio...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am kind of disappointed with this item. It w...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not only was the product just as nice as the p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Bolse AON6 battery pack is a basic medium ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I needed to replace the Siemens switch on my I...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall sentiment\n",
       "0  Works as expected - it is the five mode versio...      5.0       pos\n",
       "1  I am kind of disappointed with this item. It w...      3.0       pos\n",
       "2  Not only was the product just as nice as the p...      5.0       pos\n",
       "3  The Bolse AON6 battery pack is a basic medium ...      3.0       pos\n",
       "4  I needed to replace the Siemens switch on my I...      3.0       pos"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lo siguiente es añadir el \"sentimiento\" al dataframe mediante etiquetas, de forma que un rating menor a tres indique \n",
    "#una review \"negativa\", y un rating de 3 o más sea una review \"positiva\":\n",
    "\n",
    "#Para ello, definimos una función que etiquete cada registro según nuestros criterios:\n",
    "def label_sentiment(row):\n",
    "    if int(row['overall']) < 3:\n",
    "        return 'neg'\n",
    "    else:\n",
    "        return 'pos'\n",
    "\n",
    "#Llamamos a la función sobre una nueva columna del dataframe: \"sentiment\". Esta columna almacenará todas las etiquetas.\n",
    "df_final['sentiment'] = df_final.apply(lambda row: label_sentiment(row), axis=1)\n",
    "\n",
    "#Comprobamos el resultado:\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Con el dataframe ya etiquetado, separamos los datos en train y test:\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_final['reviewText'], # x_train y x_test -> Información a evaluar por el modelo para determinar el sentimiento.\n",
    "    df_final['sentiment'], # y_train e y_test -> Información con la que determinar si el modelo ha acertado o no en su decisión.\n",
    "    train_size=0.7, # El 70% de los datos serán de entrenamiento.\n",
    "    test_size=0.3, # El 30% de los datos serán de test.\n",
    "    random_state=42, # Fijamos la seed para asegurar la reproducibilidad del proceso.\n",
    "    shuffle=True # Barajamos los registros antes de separarlos.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el resultado en sendos csv para poder reutilizarlos en caso de tener que cerrar el notebook:\n",
    "x_train.to_csv(r'x_train_SA.csv', index = True)\n",
    "x_test.to_csv(r'x_test_SA.csv', index = True)\n",
    "y_train.to_csv(r'y_train_SA.csv', index = True)\n",
    "y_test.to_csv(r'y_test_SA.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez separados los datos, el siguiente paso es preprocesarlos y entrenar un modelo para predecir el \"sentimiento\" de cada review. Dado que buscamos una clasificación binaria (positiva o negativa), usaremos la regresión logística de scikit learn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) Entrenar un modelo sin preprocesado:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para poder trabajar con los datos de x_train, necesitaremos convertir las cadenas de texto en parámetros que \n",
    "#un modelo pueda interpretar (una matriz de features TF-IDF). Para ello, haremos una extracción de características:\n",
    "\n",
    "#Generamos la función para extraer características:\n",
    "fts = TfidfVectorizer( \n",
    "    lowercase=False, #No queremos cambio a minúsculas por defecto.\n",
    "    ngram_range=(2, 3)) #Extraeremos solo bigramas y trigramas. \n",
    "\n",
    "#Ejecutamos el vectorizador sobre el dataset de train.\n",
    "fts.fit(x_train)\n",
    "\n",
    "#Una vez tenemos la matriz, obtenemos los scores TF-IDF de la misma, que usaremos para entrenar el modelo. \n",
    "x_train_ = fts.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.9109166726926508\n",
      "Accuracy for C=0.1: 0.9109263142671198\n",
      "Accuracy for C=1: 0.9267529587581652\n",
      "Accuracy for C=10: 0.9994697134041989\n"
     ]
    }
   ],
   "source": [
    "#Para hacer regresión logística, necesitamos un parámetro de regularización (C): cuanto más pequeño sea, mayor regularización\n",
    "#aplicará el modelo. Con este grid de parámetros (c_par), podemos observar el comportamiento del modelo en cada situación\n",
    "#y seleccionar el valor óptimo:\n",
    "c_par = [0.01, 0.1, 1, 10]\n",
    "\n",
    "#Creamos una lista vacía en la que se guardarán los resultados de Accuracy del modelo sobre el dataset de train:\n",
    "train_acc = list()\n",
    "\n",
    "#Creamos una función que aplicará una regresión logística sobre el dataset por cada parámetro de C:\n",
    "for c in c_par:\n",
    "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=500) #lbfgs es un buen algoritmo para resolver problemas multiclase.\n",
    "    lr.fit(x_train_, y_train)\n",
    "    \n",
    "    #Se hará una predicción de los resultados de train:\n",
    "    train_predict = lr.predict(x_train_)\n",
    "    \n",
    "    #Imprimimos el resultado:\n",
    "    print (\"Accuracy for C={}: {}\".format(c, accuracy_score(y_train, train_predict)))\n",
    "    \n",
    "    #Añadimos a la lista vacía train_acc los resultados de Accuracy obtenidos por el modelo:\n",
    "    train_acc.append(accuracy_score(y_train, train_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) Entrenar un modelo preprocesado:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, someteremos al modelo a un preprocesamiento muy similar al utilizado en topic modeling:<br>\n",
    "<br>1: Conversión de mayúsculas a minúsculas<br>\n",
    "2: Eliminación de acentos y signos de puntuación<br>\n",
    "3: Eliminación de palabras de menos de 3 caracteres<br>\n",
    "4: Eliminación de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amfre\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "#Utilizaremos el preprocesador de Topic Modeling para tokenizar, eliminar stopwords y palabras de menos de 3 caracteres:\n",
    "def text_preprocessing(text):\n",
    "    result=[] \n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3: \n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "#Generamos la función TfidfVectorizer para extraer características:\n",
    "fts = TfidfVectorizer( \n",
    "    strip_accents='ascii', #Eliminamos los acentos y signos de puntuación con ASCII.\n",
    "    lowercase=True, #Convertimos todas las palabras a minúsculas.\n",
    "    tokenizer=text_preprocessing, #Llamamos a la función text_preprocessing para que tokenice y preprocese.\n",
    "    ngram_range=(2, 3)) #Extraeremos solo bigramas y trigramas.\n",
    "    \n",
    "#Ejecutamos el vectorizador sobre el dataset de train.\n",
    "fts.fit(x_train)\n",
    "\n",
    "#Una vez tenemos la matriz, obtenemos los scores TF-IDF de la misma, que usaremos para entrenar el modelo. \n",
    "x_train_2 = fts.transform(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.9109166726926508\n",
      "Accuracy for C=0.1: 0.9109166726926508\n",
      "Accuracy for C=1: 0.9135825680333598\n",
      "Accuracy for C=10: 0.9994841757659025\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos el modelo de la misma forma que en el caso A:\n",
    "c_par = [0.01, 0.1, 1, 10]\n",
    "\n",
    "train_acc = list()\n",
    "\n",
    "for c in c_par:\n",
    "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=500)\n",
    "    lr.fit(x_train_2, y_train)\n",
    "    \n",
    "    train_predict = lr.predict(x_train_2)\n",
    "    \n",
    "    print (\"Accuracy for C={}: {}\".format(c, accuracy_score(y_train, train_predict)))\n",
    "    \n",
    "    train_acc.append(accuracy_score(y_train, train_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) Modelo preprocesado y lemmatizado:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amfre\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "#Generamos una función a partir de text_processing que sea capaz de lemmatizar:\n",
    "def text_pl(text):\n",
    "    result=[] \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3: \n",
    "            result.append(token)\n",
    "    return [lemmatizer.lemmatize(w) for w in result]\n",
    "\n",
    "#Usamos TfidfVectorizer para extraer características:\n",
    "fts = TfidfVectorizer( \n",
    "    strip_accents='ascii', #Eliminamos los acentos y signos de puntuación con ASCII.\n",
    "    lowercase=True, #Convertimos todas las palabras a minúsculas.\n",
    "    tokenizer=text_pl, #Llamamos a la función text_pl para que tokenice, preprocese y lemmatice.\n",
    "    ngram_range=(2, 3)) #Extraeremos solo bigramas y trigramas.\n",
    "    \n",
    "#Ejecutamos el vectorizador sobre el dataset de train.\n",
    "fts.fit(x_train)\n",
    "\n",
    "#Una vez tenemos la matriz, obtenemos los scores TF-IDF de la misma, que usaremos para entrenar el modelo. \n",
    "x_train_2 = fts.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.9109166726926508\n",
      "Accuracy for C=0.1: 0.9109166726926508\n",
      "Accuracy for C=1: 0.913765757948273\n",
      "Accuracy for C=10: 0.999479354978668\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos el modelo de la misma forma que en los casos A y B:\n",
    "c_par = [0.01, 0.1, 1, 10]\n",
    "\n",
    "train_acc = list()\n",
    "\n",
    "for c in c_par:\n",
    "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=500)\n",
    "    lr.fit(x_train_2, y_train)\n",
    "    \n",
    "    train_predict = lr.predict(x_train_2)\n",
    "    \n",
    "    print (\"Accuracy for C={}: {}\".format(c, accuracy_score(y_train, train_predict)))\n",
    "    \n",
    "    train_acc.append(accuracy_score(y_train, train_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, el preprocesamiento no parece haber alterado mucho los valores de Accuracy. Esto, unido a que la Accuracy en casos de baja regularización es de prácticamente el 100%, nos hace pensar que los datos pueden sufrir cierto overfitting, sobre todo si los valores de C no son suficientemente bajos.<br>\n",
    "\n",
    "Teniendo en cuenta estos resultados, podríamos quedarnos con el modelo sin procesar por simplicidad, pero considero que es mejor preprocesar todo lo posible, pues facilitamos al modelo el trabajo en todo lo posible, y con ello mejoraremos la Accuracy en caso de que test tenga una performance más pobre, lo cual es probable. Por tanto, repetiremos el preprocesamiento del **caso C**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amfre\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "#Usamos TfidfVectorizer para extraer características:\n",
    "fts = TfidfVectorizer( \n",
    "    strip_accents='ascii', #Eliminamos los acentos y signos de puntuación con ASCII.\n",
    "    lowercase=True, #Convertimos todas las palabras a minúsculas.\n",
    "    tokenizer=text_pl, #Llamamos a la función text_pl para que tokenice, preprocese y lemmatice.\n",
    "    ngram_range=(2, 3)) #Extraeremos solo bigramas y trigramas.\n",
    "    \n",
    "#Ejecutamos el vectorizador sobre el dataset de test.\n",
    "fts.fit(x_test)\n",
    "\n",
    "#Una vez tenemos la matriz, obtenemos los scores TF-IDF de la misma, que usaremos para entrenar el modelo. \n",
    "x_test_2 = fts.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for C=0.01: 0.9126453848057412\n",
      "Accuracy for C=0.1: 0.9126453848057412\n",
      "Accuracy for C=1: 0.9134327686666217\n",
      "Accuracy for C=10: 0.9997412881599964\n"
     ]
    }
   ],
   "source": [
    "#Observamos cómo funciona el modelo sobre los datos de test:\n",
    "c_par = [0.01, 0.1, 1, 10] \n",
    "\n",
    "test_acc = list()\n",
    "\n",
    "for c in c_par:\n",
    "    lr = LogisticRegression(C=c, solver='lbfgs', max_iter=500)\n",
    "    lr.fit(x_test_2, y_test)\n",
    "    \n",
    "    test_predict = lr.predict(x_test_2)\n",
    "    \n",
    "    print (\"Accuracy for C={}: {}\".format(c, accuracy_score(y_test, test_predict)))\n",
    "    \n",
    "    test_acc.append(accuracy_score(y_test, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Accuracy de los datos de test es muy similar a la de los datos de train, lo cual indicaría que el modelo es bastante eficaz a la hora de determinar si las reviews son positivas o negativas, y descartaría a priori la posibilidad de overfitting.<br>\n",
    "\n",
    "La limitación principal de este modelo es que no tiene en cuenta las ambigüedades o neutralidades, es decir, las reviews de 3 estrellas, que en principio indican que el producto tiene cosas buenas y malas. Una manera de mejorar las prestaciones del modelo sería añadir una tercera etiqueta (neutral) y observar entonces cómo se comporta.<br>\n",
    "\n",
    "También podríamos explorar otros modelos de clasificación para análisis de sentimiento, pues existen varios [papers](https://paperswithcode.com/task/sentiment-analysis) y [artículos](https://towardsdatascience.com/sentiment-analysis-for-text-with-deep-learning-2f0a0c6472b5) que cuentan con modelos ya preparados y entrenados que podrían ser más eficaces, o incluso permitir clasificaciones más complejas que se nos pudieran ocurrir (por ejemplo, una etiqueta para cada rating/estrella)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
